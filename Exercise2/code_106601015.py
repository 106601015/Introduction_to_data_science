from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import sklearn.metrics
import numpy as np
import math


class Node:
    """A decision tree node."""
    def __init__(self, gini, entropy, num_samples,
            num_samples_per_class, predicted_class):
        self.gini = gini
        self.entropy = entropy
        self.num_samples = num_samples
        self.num_samples_per_class = num_samples_per_class
        self.predicted_class = predicted_class
        self.feature_index = 0
        self.threshold = 0
        self.left = None
        self.right = None

class DecisionTreeClassifier:
    def __init__(self,criterion='gini', max_depth=4):
        self.criterion = criterion
        self.max_depth = max_depth

    # culculate gini
    def _gini(self,sample_y,n_classes):
        gini = 1
        d = np.size(sample_y)
        for index in range(n_classes):
            di = np.count_nonzero(sample_y == index)
            p_i = (di/d)**2
            gini-=p_i
            #print('-->', index, di, p_i)
        #print('gini=', gini, 'sample_y size:', d)

        return gini

    # # culculate entropy
    def _entropy(self,sample_y,n_classes):
        entropy = 0
        d = np.size(sample_y)
        for index in range(n_classes):
            di = np.count_nonzero(sample_y == index)
            p_i = (di/d)**2
            #print('==>', index, di, p_i)
            try:entropy += -p_i*math.log(p_i, 2)
            except ValueError:pass
        #print('entropy=', entropy, 'sample_y size:', d)

        return entropy

    #gini tree over
    def _feature_split(self, X, y,n_classes):
        print('feature_split init')
        # Returns:
        #  best_idx: Index of the feature for best split, or None if no split is found.
        #  best_thr: Threshold to use for the split, or None if no split is found.
        m = y.size
        if m <= 1:
            return None, None

        # Gini or Entropy of current node.
        if self.criterion == "gini":
            best_criterion = self._gini(y,n_classes) #洧냨洧녰洧녵洧녰(洧냥)
        else:
            best_criterion = self._entropy(y,n_classes) #Info(洧냥)

        best_idx, best_thr = None, None
        best_thr_location = 0 # just for test

        # TODO: find the best split, loop through all the features, and consider all the
        # midpoints between adjacent training samples as possible thresholds.
        # Computethe Gini or Entropy impurity of the split generated by that particular feature/threshold
        # pair, and return the pair with smallest impurity.

        if self.criterion == "gini":
            # loop through all the features.
            for feature_index in range(X.shape[1]):
                print('feature_index', feature_index)
                X_raw = X[:, feature_index]

                # sort X_raw to X_order
                X_order_index_list = np.argsort(X_raw, kind='quicksort')
                X_order = X_raw[X_order_index_list]
                y_order = y[X_order_index_list]

                # init lowest_gini(need less than best_criterion(all y))
                lowest_gini = best_criterion

                # consider all the midpoints between adjacent training samples as possible thresholds.
                for thr in range(1, X_order.shape[0]):
                    # culculate 洧냨洧녰洧녵洧녰(洧냥洧녰) and 洧냨洧녰洧녵洧녰_洧냢(洧냥)
                    gini_left = self._gini(y_order[:thr], self.n_classes_)
                    gini_right = self._gini(y_order[thr:], self.n_classes_)
                    gini_split = (thr/X_order.shape[0])*gini_left + ((X_order.shape[0]-thr)/X_order.shape[0])*gini_right

                    if gini_split < lowest_gini:
                        lowest_gini = gini_split
                        best_idx, best_thr = feature_index, X_order[thr]
                        best_thr_location = thr

            #print('best_idx, best_thr, best_thr_location', best_idx, best_thr, best_thr_location)
            #print(y_order, best_thr_location, y_order[best_thr_location])
            #print(X_order, best_thr_location, X_order[best_thr_location])
            print('using gini, best_idx and best_thr and best_thr_location:', best_idx, best_thr, best_thr_location)

        else:
            # loop through all the features.
            for feature_index in range(X.shape[1]):
                #print('feature_index', feature_index)
                X_raw = X[:, feature_index]

                # sort X_raw to X_order
                X_order_index_list = np.argsort(X_raw, kind='quicksort')
                X_order = X_raw[X_order_index_list]
                y_order = y[X_order_index_list]

                # init lowest_entropy(need less than best_criterion(all y))
                lowest_entropy = best_criterion

                # consider all the midpoints between adjacent training samples as possible thresholds.
                for thr in range(1, X_order.shape[0]):
                    # culculate Info(洧냥洧녰) and Info_洧냢(洧냥)
                    entropy_left = self._entropy(y_order[:thr], self.n_classes_)
                    entropy_right = self._entropy(y_order[thr:], self.n_classes_)
                    entropy_split = (thr/X_order.shape[0])*entropy_left + ((X_order.shape[0]-thr)/X_order.shape[0])*entropy_right

                    if entropy_split < lowest_entropy:
                        lowest_entropy = entropy_split
                        best_idx, best_thr = feature_index, X_order[thr]
                        best_thr_location = thr

            #print('best_idx, best_thr, best_thr_location', best_idx, best_thr, best_thr_location)
            #print(y_order, best_thr_location, y_order[best_thr_location])
            #print(X_order, best_thr_location, X_order[best_thr_location])
            print('using entropy, best_idx and best_thr and best_thr_location:', best_idx, best_thr, best_thr_location)

        return best_idx, best_thr

    def _build_tree(self, X, y, depth=0):
        print('build_tree init')
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        #print('num_samples_per_class, predicted_class', num_samples_per_class, predicted_class)
        node = Node(
            gini=self._gini(y,self.n_classes_),
            entropy = self._entropy(y,self.n_classes_),
            num_samples=y.size,
            num_samples_per_class=num_samples_per_class,
            predicted_class=predicted_class,
        )

        if depth < self.max_depth:
            idx, thr = self._feature_split(X, y,self.n_classes_)
            if idx is not None:
            # TODO: Split the tree recursively according index and threshold until maximum depth is reached.
                # using thr to split data
                left_num = np.count_nonzero(X[:, idx] < thr)
                right_num = np.count_nonzero(X[:, idx] >= thr)
                # thr is the leftest or the rightest, means you dont have to split
                if left_num==0 or right_num==0:
                    print('!!!!!thr is the leftest or the rightest!!!!!')
                    pass
                else:
                    # split data
                    X_left = np.zeros((left_num, X.shape[1]))
                    X_right = np.zeros((right_num, X.shape[1]))
                    y_left = np.zeros((left_num))
                    y_right = np.zeros((right_num))
                    print('split to:', X_left.shape, X_right.shape)

                    left_con, right_con, all_con = 0, 0, 0
                    for case in X:
                        if case[idx] < thr:
                            X_left[left_con] = case
                            y_left[left_con] = y[all_con]
                            left_con+=1
                        else:
                            X_right[right_con] = case
                            y_right[right_con] = y[all_con]
                            right_con+=1
                        all_con+=1
                    #print(idx, thr, X_left, X_right, y_left, y_right)

                    # depth+=1, create left and right leaf
                    depth+=1
                    print('left<---------------')
                    node.left = self._build_tree(X_left, y_left, depth)
                    print('--------------->right')
                    node.right = self._build_tree(X_right, y_right, depth)
                    node.feature_index = idx
                    node.threshold = thr

        return node

    def fit(self,X,Y):
        print('fit init')
        # Fits to the given training data
        self.n_classes_ = len(np.unique(Y))
        self.n_features_ = X.shape[1]

        # if user entered a value which was neither gini nor entropy
        if self.criterion != 'gini' :
            if self.criterion != 'entropy':
                self.criterion='gini'
        self.tree_ = self._build_tree(X, Y)

    def predict(self,X):
        print('\npredict init')
        pred = []
        #TODO: predict the label of data
        for x in X:
            node = self.tree_
            while node.left != None:
                if x[node.feature_index] < node.threshold:
                    node = node.left
                else:
                    node = node.right
            pred.append(node.predicted_class)
        return pred

def load_train_test_data(test_ratio=.3, random_state = 1):
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target
    sc = StandardScaler()
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size = test_ratio, random_state=random_state, stratify=y)
    return X_train, X_test, y_train, y_test

def scale_features(X_train, X_test):
    sc = StandardScaler()
    sc.fit(X_train)
    X_train_std = sc.transform(X_train)
    X_test_std = sc.transform(X_test)
    return X_train_std , X_test_std

def accuracy_report(X_train_scale, y_train,X_test_scale,y_test,criterion='gini',max_depth=4):
    tree = DecisionTreeClassifier(criterion = criterion, max_depth=max_depth)
    tree.fit(X_train_scale, y_train)

    pred = tree.predict(X_train_scale)
    print(criterion + " tree train accuracy: %f"
        % (sklearn.metrics.accuracy_score(y_train, pred )))
    pred = tree.predict(X_test_scale)
    print(criterion + " tree test accuracy: %f"
        % (sklearn.metrics.accuracy_score(y_test, pred )))



def main():
    X_train, X_test, y_train, y_test = load_train_test_data(test_ratio=.3,random_state=1)
    X_train_scale, X_test_scale = scale_features(X_train, X_test)

    # gini tree
    accuracy_report(X_train_scale, y_train,X_test_scale,y_test,criterion='gini',max_depth=4)
    print('gini tree over\n\n')
    # entropy tree
    accuracy_report(X_train_scale, y_train,X_test_scale,y_test,criterion='entropy',max_depth=4)
    print('entropy tree over')

if __name__ == "__main__":
    main()
